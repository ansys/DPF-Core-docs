{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Distributed post without client connection to remote processes with Operators\n",
    "This example shows how distributed files can be read and post processed\n",
    "on distributed processes. After remote post processing, results a merged\n",
    "on the local process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dpf module and its examples files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from ansys.dpf import core as dpf\n",
    "from ansys.dpf.core import examples\n",
    "from ansys.dpf.core import operators as ops\n",
    "from ansys.jupyterhub.manager import spawn_dpf, delete_pod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the template workflow of total displacement\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure the servers\n",
    "Start two dpf servers. Workflows instances will be created on each of those servers to\n",
    "address each a different result file.\n",
    "In this example, we will post process an analysis distributed in 2 files,\n",
    "we will consequently require 2 remote processes\n",
    "To make this example easier, we will start local servers here,\n",
    "but we could get connected to any existing servers on the network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "server1, pod1_name = spawn_dpf()\n",
    "server2, pod2_name = spawn_dpf()\n",
    "servers = [server1, server2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we show how we could send files in temporary directory if we were not\n",
    "in shared memory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "local_files = examples.download_distributed_files()\n",
    "server_file_paths = [dpf.upload_file_in_tmp_folder(local_files[0], server=server1),\n",
    "                     dpf.upload_file_in_tmp_folder(local_files[1], server=server2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Send workflows on servers\n",
    "Here we create new instances on the server by copies of the template workflow\n",
    "We also connect the data sources to those workflows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "remote_operators = []\n",
    "for i, server in enumerate(servers):\n",
    "    displacement = ops.result.displacement(server=server)\n",
    "    norm = ops.math.norm_fc(displacement, server=server)\n",
    "    remote_operators.append(norm)\n",
    "    ds = dpf.DataSources(server_file_paths[i], server=server)\n",
    "    displacement.inputs.data_sources(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a workflow able to merge the results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "server3, pod3_name = spawn_dpf()\n",
    "workflow = dpf.Workflow(server=server3)\n",
    "merge = ops.utility.merge_fields_containers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect the workflows together and get the output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for i, server in enumerate(servers):\n",
    "    merge.connect(i, remote_operators[i], 0)\n",
    "\n",
    "fc = merge.get_output(0, dpf.types.fields_container)\n",
    "print(fc)\n",
    "print(fc[0].min().data)\n",
    "print(fc[0].max().data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_pod(pod1_name)\n",
    "delete_pod(pod2_name)\n",
    "delete_pod(pod3_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
