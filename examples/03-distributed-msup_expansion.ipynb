{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Distributed modal superposition\n",
    "This example shows how distributed files can be read and expanded\n",
    "on distributed processes. The modal basis (2 distributed files) is read\n",
    "on 2 remote servers and the modal response reading and the expansion is\n",
    "done on a third server.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dpf module and its examples files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from ansys.dpf import core as dpf\n",
    "from ansys.dpf.core import examples\n",
    "from ansys.dpf.core import operators as ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the template workflow\n",
    "this workflow will provide the modal basis and the mesh for each domain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "server1 = dpf.start_local_server(as_global=True)\n",
    "template_workflow = dpf.Workflow()\n",
    "displacement = ops.result.displacement()\n",
    "mesh = ops.mesh.mesh_provider()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the operators to the template workflow and name its inputs and outputs\n",
    "Once workflow's inputs and outputs are named, they can be connected later on\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "template_workflow.add_operators([displacement])\n",
    "template_workflow.set_input_name(\"data_sources\", displacement.inputs.data_sources)\n",
    "template_workflow.set_input_name(\"data_sources\", mesh.inputs.data_sources)\n",
    "template_workflow.set_output_name(\"out\", displacement.outputs.fields_container)\n",
    "template_workflow.set_output_name(\"outmesh\", mesh.outputs.mesh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure the servers\n",
    "Start two more dpf servers. Workflows instances will be created on each of those servers to\n",
    "address each a different result file.\n",
    "In this example, we will post process an analysis distributed in 2 files,\n",
    "we will consequently require 2 remote processes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server2 = dpf.start_local_server(as_global=False)\n",
    "server3 = dpf.start_local_server(as_global=False)\n",
    "servers = [server2, server3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the file path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "base_path = examples.distributed_msup_folder\n",
    "files = [base_path + r'/file0.mode', base_path + r'/file1.mode']\n",
    "files_aux = [base_path + r'/file0.rst', base_path + r'/file1.rst']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload files to servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_file_paths = [dpf.upload_file_in_tmp_folder(files[0], server=server2),\n",
    "                     dpf.upload_file_in_tmp_folder(files[1], server=server3)]\n",
    "server_file_aux_paths = [dpf.upload_file_in_tmp_folder(files_aux[0], server=server2),\n",
    "                         dpf.upload_file_in_tmp_folder(files_aux[1], server=server3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Send workflows on servers\n",
    "Here we create new instances on the server by copies of the template workflow\n",
    "We also connect the data sources to those workflows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "remote_workflows = []\n",
    "for i, server in enumerate(servers):\n",
    "    remote_workflows.append(template_workflow.create_on_other_server(server))\n",
    "    ds = dpf.DataSources(server_file_paths[i])\n",
    "    ds.add_file_path(server_file_aux_paths[i])\n",
    "    remote_workflows[i].connect(\"data_sources\", ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a workflow for expansion\n",
    "In this workflow we merge the modal basis, the meshes, read the modal response\n",
    "and expand the modal response with the modal basis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "workflow = dpf.Workflow()\n",
    "merge = ops.utility.merge_fields_containers()\n",
    "merge_mesh = ops.utility.merge_meshes()\n",
    "\n",
    "local_file = base_path + r'/file_load_1.rfrq'\n",
    "server_file_path = dpf.upload_file_in_tmp_folder(local_file)\n",
    "ds = dpf.DataSources(server_file_path)\n",
    "response = ops.result.displacement(data_sources=ds)\n",
    "response.inputs.mesh(merge_mesh.outputs.merges_mesh)\n",
    "\n",
    "expansion = ops.math.modal_superposition(solution_in_modal_space=response, modal_basis=merge)\n",
    "component = ops.logic.component_selector_fc(expansion, 1)\n",
    "\n",
    "workflow.add_operators([merge, response, expansion, merge_mesh, component])\n",
    "workflow.set_input_name(\"in0\", merge, 0)\n",
    "workflow.set_input_name(\"in1\", merge, 1)\n",
    "workflow.set_input_name(\"inmesh0\", merge_mesh, 0)\n",
    "workflow.set_input_name(\"inmesh1\", merge_mesh, 1)\n",
    "\n",
    "workflow.set_output_name(\"expanded\", component.outputs.fields_container)\n",
    "workflow.set_output_name(\"mesh\", merge_mesh.outputs.merges_mesh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect the workflows together and get the output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for i, server in enumerate(servers):\n",
    "    workflow.connect_with(remote_workflows[i],\n",
    "                            {\"out\": \"in\" + str(i), \"outmesh\": \"inmesh\" + str(i)})\n",
    "\n",
    "fc = workflow.get_output(\"expanded\", dpf.types.fields_container)\n",
    "merged_mesh = workflow.get_output(\"mesh\", dpf.types.meshed_region)\n",
    "merged_mesh.plot(fc.get_field_by_time_complex_ids(1, 0))\n",
    "merged_mesh.plot(fc.get_field_by_time_complex_ids(10, 0))\n",
    "print(fc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
